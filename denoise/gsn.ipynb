{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gsn.perform_gsn import perform_gsn\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d, uniform_filter1d\n",
    "from scipy.stats import pearsonr, spearmanrho\n",
    "\n",
    "# raw: (units, time, images, trials)\n",
    "def psth_gaussian(raw, sigma=3):\n",
    "    # smooth along the time axis\n",
    "    psth = gaussian_filter1d(raw, sigma=sigma, axis=1)\n",
    "    return psth\n",
    "\n",
    "def psth_window(raw, window_ms=50):\n",
    "    # raw: (units, time, images, trials)\n",
    "    # convert NaN to 0 so windowing behaves properly\n",
    "    X = np.nan_to_num(raw, nan=0.0)\n",
    "    # sliding window average over time axis\n",
    "    psth = uniform_filter1d(X, size=window_ms, axis=1, mode='constant')\n",
    "    return psth\n",
    "\n",
    "def summary(x):\n",
    "    print(f'''Summary statistics {x.shape}:\n",
    "    \\tMedian: {np.nanmedian(x)}\n",
    "    \\tMean: {np.nanmean(x)}\n",
    "    \\tMinimum: {np.nanmin(x)}\n",
    "    \\tMaximum:{np.nanmax(x)}\\n''')\n",
    "\n",
    "def segregate(df, method='truncate'):\n",
    "    roi_arrays = {}\n",
    "    for roi, df_roi in df.groupby('roi'):\n",
    "        arrays = df_roi['raster'].to_list()\n",
    "        if method == 'truncate':\n",
    "            # count valid trials per unit\n",
    "            valid_counts = []\n",
    "            valid_masks  = []\n",
    "\n",
    "            # minimum trials with no NaNs\n",
    "            for a in arrays:\n",
    "                vm = ~np.isnan(a).any(axis=(0,1))\n",
    "                valid_masks.append(vm)\n",
    "                valid_counts.append(vm.sum())\n",
    "            min_T = min(valid_counts)\n",
    "        \n",
    "            total = []\n",
    "            for a, vm in zip(arrays, valid_masks):\n",
    "                idx = np.where(vm)[0][:min_T]   # first min_T valid trials\n",
    "                total.append(a[:, :, idx])\n",
    "        elif method == 'pad':\n",
    "            # find max number of trials *within this ROI*\n",
    "            max_T = max(a.shape[2] for a in arrays)\n",
    "            total = []\n",
    "            for a in arrays:\n",
    "                T = a.shape[2]\n",
    "                if T < max_T:\n",
    "                    pad_width = ((0, 0), (0, 0), (0, max_T - T))\n",
    "                    a = np.pad(a, pad_width, constant_values=np.nan)\n",
    "                total.append(a)\n",
    "\n",
    "        x_roi = np.array(total)   # (units, 450, 1072, min_T)\n",
    "        roi_arrays[roi] = x_roi\n",
    "        print(roi, x_roi.shape)\n",
    "\n",
    "    return roi_arrays\n",
    "\n",
    "def split_half_reliability_sliding(\n",
    "    X,                       # [units, time, stimuli, reps]\n",
    "    bin_ms=10,               # 10 ms sliding window\n",
    "    onset=50,                # index offset for stim onset (your ONSET)\n",
    "    start_range=(20, 200),   # ms post-onset\n",
    "    end_range=(90, 390),     # ms post-onset\n",
    "    min_window_ms=10,        # enforce end > start\n",
    "    random_state=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    for each unit, search start/end windows (10 ms steps) and return:\n",
    "    - best window (start_ms, end_ms)\n",
    "    - best split-half reliability (pearson r across stimuli)\n",
    "    notes:\n",
    "    - splits trials into two halves per stimulus (reps must be even-ish; we use floor/ceil)\n",
    "    - computes mean over time-window, then mean over trials within half, per stimulus\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    U, T, S, R = X.shape\n",
    "    assert R >= 4, 'need >=4 reps to split into two halves with >=2 reps each'\n",
    "\n",
    "    # convert ms ranges to index ranges\n",
    "    start_idxs = np.arange(onset + start_range[0], onset + start_range[1] + 1, bin_ms)\n",
    "    end_idxs   = np.arange(onset + end_range[0],   onset + end_range[1] + 1, bin_ms)\n",
    "\n",
    "    # precompute time-averaged responses for every possible [t0,t1) window is expensive;\n",
    "    # instead, use cumulative sum over time for O(1) window means.\n",
    "    # X_mean_time: [U, T, S, R]\n",
    "    csum = np.cumsum(X, axis=1, dtype=np.float64)  # cumulative over time\n",
    "\n",
    "    def time_window_mean(t0, t1):\n",
    "        # mean over time axis in [t0, t1) using cumsum; returns [U, S, R]\n",
    "        # guard: t1 > t0\n",
    "        num = csum[:, t1 - 1] - (csum[:, t0 - 1] if t0 > 0 else 0.0)\n",
    "        return num / (t1 - t0)\n",
    "\n",
    "    best_r = np.full(U, -np.inf, dtype=np.float64)\n",
    "    best_win = np.zeros((U, 2), dtype=int)\n",
    "\n",
    "    # trial split indices per stimulus, per unit are the same (random split per unit per eval is fine).\n",
    "    # simplest: for each unit, we resample splits per window (matches \"randomly split\").\n",
    "    # if you want one split reused across windows, move the split generation outside the loops.\n",
    "\n",
    "    for t0 in start_idxs:\n",
    "        for t1 in end_idxs:\n",
    "            if t1 <= t0 + (min_window_ms - 1):\n",
    "                continue\n",
    "            if t0 < 0 or t1 > T:\n",
    "                continue\n",
    "\n",
    "            # [U, S, R] window-averaged responses per trial\n",
    "            Y = time_window_mean(t0, t1)\n",
    "\n",
    "            # split-half reliability per unit\n",
    "            r_here = np.full(U, np.nan, dtype=np.float64)\n",
    "\n",
    "            for u in range(U):\n",
    "                # per stimulus, randomly permute reps then split into halves\n",
    "                # produce two vectors of length S: mean over reps in each half\n",
    "                v1 = np.empty(S, dtype=np.float64)\n",
    "                v2 = np.empty(S, dtype=np.float64)\n",
    "\n",
    "                for s in range(S):\n",
    "                    perm = rng.permutation(R)\n",
    "                    h = R // 2\n",
    "                    a = perm[:h]\n",
    "                    b = perm[h:]\n",
    "\n",
    "                    m1 = np.mean(Y[u, s, a])\n",
    "                    m2 = np.mean(Y[u, s, b])\n",
    "                    v1[s] = m1\n",
    "                    v2[s] = m2\n",
    "\n",
    "                # correlation across stimuli (nan-safe)\n",
    "                v1m = v1 - v1.mean()\n",
    "                v2m = v2 - v2.mean()\n",
    "                denom = np.sqrt((v1m**2).sum() * (v2m**2).sum())\n",
    "                r_here[u] = (v1m @ v2m) / denom if denom > 0 else np.nan\n",
    "\n",
    "            # update best per unit\n",
    "            better = r_here > best_r\n",
    "            best_r[better] = r_here[better]\n",
    "            best_win[better, 0] = t0 - onset\n",
    "            best_win[better, 1] = t1 - onset\n",
    "\n",
    "    return best_r, best_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ras_df = pd.read_pickle('../../datasets/NNN/trial_raster_data_Unknown19F.pkl')\n",
    "print(f'Succesfully loaded data for {len(ras_df)} units.')\n",
    "\n",
    "roi_arrays = segregate(ras_df, method='truncate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref = pd.read_pickle('./../../datasets/NNN/unit_data_full.pkl')\n",
    "# roi_ref = ref[ref['roi']=='Unknown_19_F'].reset_index(drop=True)\n",
    "# roi_ref['session'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input to the model was a unit × stimuli × trial matrix\n",
    "# these are only single unit data\n",
    "ROI = 'Unknown_19_F'\n",
    "RESP = (100, 250) # this is just a random set for now\n",
    "BASE = (-25, 25)\n",
    "ONSET = 50\n",
    "\n",
    "RESP = slice(ONSET + RESP[0], ONSET + RESP[1])\n",
    "BASE = slice(ONSET + BASE[0], ONSET + BASE[1])\n",
    "\n",
    "# shape: (74, 450, 1072, reps) [ie. unit x time x stimuli x trial matrix]\n",
    "dat = roi_arrays[ROI] \n",
    "\n",
    "# shape: (74, 1072, reps) [ie. unit x stimuli x trial matrix]\n",
    "dat_tavg = np.mean(dat[:, RESP, :, :], axis=1) - np.mean(dat[:, BASE, :, :], axis=1)\n",
    "print(dat_tavg.shape)\n",
    "\n",
    "# results = perform_gsn(dat_tavg, {'wantshrinkage': True})\n",
    "\n",
    "# # Let's take a look at ncsnr\n",
    "# plt.figure()\n",
    "# plt.hist(results['ncsnr'])\n",
    "# plt.xlabel('Noise ceiling SNR')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# # Notice that the data covariance estimate involved some shrinkage:\n",
    "# print(results['shrinklevelD'])\n",
    "\n",
    "# # Notice that the noise covariance estimate in this case did not involve shrinkage:\n",
    "# print(results['shrinklevelN'])\n",
    "\n",
    "# # sns.heatmap(results['cSb'], vmax=np.max(results['cSb']), vmin=np.min(results['cSb']))\n",
    "# sns.heatmap(results['cSb'], vmax=0.000005, vmin=0)\n",
    "# summary(results['cSb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage\n",
    "X = roi_arrays[ROI]  # [units, time, stimuli, reps]\n",
    "best_r, best_win = split_half_reliability_sliding(\n",
    "    X,\n",
    "    bin_ms=10,\n",
    "    onset=50,\n",
    "    start_range=(20, 200),\n",
    "    end_range=(90, 390),\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# best_win is in ms post-onset (because we subtract ONSET at the end)\n",
    "# each row: [start_ms, end_ms]\n",
    "print('best r (first 5):', best_r[:5])\n",
    "print('best windows (first 5):', best_win[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('non-zero entries', np.sum(np.mean(X, axis=3)>0))\n",
    "print('total entries', X.shape[0]*X.shape[1]*X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = roi_arrays[ROI]  # [units, time, stimuli, reps]\n",
    "scaling_factor = 1000\n",
    "\n",
    "best_responses = []\n",
    "for unit in range(len(X)):\n",
    "    window = best_win[unit]\n",
    "    unit_resp = X[unit, window[0]:window[1], :, :]\n",
    "    baseline = X[unit, 25:75, :, :]\n",
    "\n",
    "    baseline_subtracted = np.mean(unit_resp, axis=0) - np.mean(baseline, axis=0)\n",
    "    best_responses.append(baseline_subtracted)\n",
    "\n",
    "best_responses = np.array(best_responses)\n",
    "scaled = best_responses*scaling_factor\n",
    "\n",
    "results = perform_gsn(best_responses, {'wantshrinkage': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ROI = 'Unknown_19_F'\n",
    "x = roi_arrays[ROI] # shape: (608, 450, 1072, reps)\n",
    "psth = psth_window(x, window_ms=50)\n",
    "summary(psth)\n",
    "\n",
    "normed = psth / np.max(psth)\n",
    "summary(normed)\n",
    "print(normed.shape)\n",
    "\n",
    "# average over images\n",
    "img_avg = np.mean(normed, axis=2)\n",
    "# average over trials\n",
    "img_avg_g = np.mean(img_avg, axis=2)\n",
    "print(img_avg_g.shape)\n",
    "# xt = x\n",
    "# out = xt / np.nanmax(xt)\n",
    "# # summary(out)\n",
    "# # print(out.shape)\n",
    "# psth = psth_window(out, window_ms=50)\n",
    "\n",
    "# Perform GSN.\n",
    "# We explicitly set the wantshrinkage flag to true.\n",
    "results = perform_gsn(img_avg, {'wantshrinkage': True})\n",
    "\n",
    "# Let's take a look at ncsnr\n",
    "plt.figure()\n",
    "plt.hist(results['ncsnr'])\n",
    "plt.xlabel('Noise ceiling SNR')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Notice that the data covariance estimate involved some shrinkage:\n",
    "print(results['shrinklevelD'])\n",
    "\n",
    "# Notice that the noise covariance estimate in this case did not involve shrinkage:\n",
    "print(results['shrinklevelN'])\n",
    "\n",
    "# # sns.heatmap(results['cSb'], vmax=np.max(results['cSb']), vmin=np.min(results['cSb']))\n",
    "# sns.heatmap(results['cSb'], vmax=0.000005, vmin=0)\n",
    "# summary(results['cSb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigcov = results['cSb']\n",
    "noisecov = results['cNb']\n",
    "\n",
    "# Set range for color limits\n",
    "# vrange = [np.nanmin(results['cSb']), np.nanmax(results['cSb'])]\n",
    "vrange = [np.nanmean(sigcov) - 3*np.nanstd(sigcov), np.nanmean(sigcov) + 3*np.nanstd(sigcov)]\n",
    "# vrange = [-0.1, 0.1]\n",
    "\n",
    "\n",
    "# Visualize the covariance estimates\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Noise covariance estimate\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(results['cN'], vmin=vrange[0], vmax=vrange[1], aspect='auto', interpolation='none', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('cN')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Final noise covariance estimate\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(results['cNb'], vmin=vrange[0], vmax=vrange[1], aspect='auto', interpolation='none', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('cNb\\n(Final noise covariance estimate)')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Signal covariance estimate\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(results['cS'], vmin=vrange[0], vmax=vrange[1], aspect='auto', interpolation='none', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('cS')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Final signal covariance estimate\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(results['cSb'], vmin=vrange[0], vmax=vrange[1], aspect='auto', interpolation='none', cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('cSb\\n(Final signal covariance estimate)')\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sigcov = results['cSb']\n",
    "noisecov = results['cNb']\n",
    "\n",
    "print(scaling_factor)\n",
    "triu = np.triu_indices_from(sigcov, k=1)\n",
    "print(f'Signal-noise correlation: \\n\\t{pearsonr(sigcov[triu], noisecov[triu]).statistic:02e}')\n",
    "print(f'Mean off-diagonal: \\n\\t{np.nanmean(sigcov[triu]):02e} (signal), \\n\\t{np.nanmean(noisecov[triu]):02e} (noise)')\n",
    "print(f'Mean off-diagonal x 1000: \\n\\t{np.nanmean(sigcov[triu])*1000:02e} (signal), \\n\\t{np.nanmean(noisecov[triu])*1000:02e} (noise)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(sigcov) + 3*np.nanstd(sigcov)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv-py3 (manifold-dynamics)",
   "language": "python",
   "name": "manifold-dynamics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
