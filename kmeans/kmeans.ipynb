{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import zscore, pearsonr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dat = pd.read_pickle('../../datasets/NNN/unit_data_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "ONSET_MS = 0          # stimulus onset (ms)\n",
    "WINDOW_MS = 350        # analyze 0-300 ms post onset\n",
    "BIN_MS = 1             # temporal bin size (ms). If not 1, change indices below\n",
    "K_RANGE = range(2, 11) # candidates for number of clusters\n",
    "RANDOM_STATE = 0\n",
    "N_INIT = \"auto\"        # or an int (e.g., 20) if on older sklearn\n",
    "MAX_IMAGES_FOR_RSA = None   # to keep RSA runtime reasonable. None for all images\n",
    "RSA_METRIC = \"correlation\"       # correlation type when comparing RDM vectors\n",
    "SAVE_PREFIX = \"clustering_out\"\n",
    "\n",
    "SAVE_DIR = './../../../buckets/manifold-dynamics/raw-data'\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# (optional) fix y-lims to avoid flicker\n",
    "# ylo, yhi = -0.5, 2.0\n",
    "\n",
    "for i in tqdm(np.random.choice(range(len(dat)), size=50)):\n",
    "    row = dat.iloc[i]\n",
    "    A = np.asarray(row[\"img_psth\"])\n",
    "    avg1 = A.mean(axis=1)\n",
    "    avg2 = np.asarray(row[\"avg_psth\"])\n",
    "\n",
    "    ax.cla()\n",
    "    sns.lineplot(x=np.arange(len(avg1)), y=avg1, ax=ax, label=\"mean over images\")\n",
    "    sns.lineplot(x=np.arange(len(avg2)), y=avg2, ax=ax, label=\"avg_psth\")\n",
    "    ax.set_title(f\"Unit {i}\")\n",
    "    ax.set_xlabel(\"Time (ms)\")\n",
    "    ax.set_ylabel(\"FR (a.u.)\")\n",
    "    ax.legend()\n",
    "    # ax.set_ylim(ylo, yhi)\n",
    "\n",
    "    display(fig)               # draw current frame\n",
    "    time.sleep(0.4)            # hold on screen\n",
    "    clear_output(wait=True)    # erase before next frame\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unit_timecourse(row, start=None, end=None):\n",
    "    \"\"\"\n",
    "    Return the unit's avg PSTH within the analysis window.\n",
    "    If avg_psth is missing, derive it by averaging img_psth across images.\n",
    "    Ensures shape (T,) where T=end-start.\n",
    "    \"\"\"\n",
    "    avg = row['avg_psth']\n",
    "    if avg is None or (isinstance(avg, float) and np.isnan(avg)):\n",
    "        A = np.asarray(row['img_psth'])  # (time, images)\n",
    "        if A.ndim != 2:\n",
    "            raise ValueError(\"img_psth must be 2D (time x images)\")\n",
    "        avg = A.mean(axis=1)\n",
    "    avg = np.asarray(avg)\n",
    "    if avg.ndim != 1:\n",
    "        raise ValueError(\"avg_psth must be 1D (time,)\")\n",
    "    # take all values if start/end is not specified\n",
    "    if start is None:\n",
    "        start = 0\n",
    "        end = len(avg)\n",
    "    if len(avg) < end:\n",
    "        raise ValueError(f\"avg_psth length {len(avg)} < required end index {end}\")\n",
    "    return avg[start:end]  # (T,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unit_idx = 500           # <-- pick your unit\n",
    "N_SHOW = 500            # image psth's to show\n",
    "\n",
    "A = np.asarray(dat.iloc[unit_idx][\"img_psth\"])  # shape: (time, images)\n",
    "T, I = A.shape\n",
    "time_axis = np.arange(T) * BIN_MS\n",
    "\n",
    "# pick some random images (or your own indices)\n",
    "rng = np.random.default_rng(0)\n",
    "img_ids = np.sort(rng.choice(I, size=N_SHOW, replace=False))\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "# ---------- Full view ----------\n",
    "ax = axes[0]\n",
    "for img in img_ids:\n",
    "    sns.lineplot(x=time_axis, y=A[:, img], alpha=0.3, lw=1.0, ax=ax)\n",
    "ax.plot(time_axis, A[:, img_ids].mean(axis=1), color='k', lw=2.5, label='Mean')\n",
    "ax.axvline(50, ls=\"--\", color=\"k\", alpha=0.5)\n",
    "ax.set_title(\"Unit PSTHs (full scale)\")\n",
    "ax.set_ylabel(\"Firing rate (a.u.)\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# ---------- Zoomed view ----------\n",
    "ax = axes[1]\n",
    "for img in img_ids:\n",
    "    sns.lineplot(x=time_axis, y=A[:, img], alpha=0.2, lw=1.0, ax=ax)\n",
    "mean_trace = A[:, img_ids].mean(axis=1)\n",
    "ax.plot(time_axis, mean_trace, color='k', lw=2.5)\n",
    "ax.axvline(50, ls=\"--\", color=\"k\", alpha=0.5)\n",
    "ax.set_title(\"Zoomed on mean response\")\n",
    "ax.set_xlabel(\"Time (ms)\")\n",
    "ax.set_ylabel(\"Firing rate (a.u.)\")\n",
    "\n",
    "# Zoom in on the y-axis around the mean trace\n",
    "margin = 0.2 * (mean_trace.max() - mean_trace.min())\n",
    "ax.set_ylim(mean_trace.min() - margin, mean_trace.max() + margin)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = np.asarray(dat.iloc[unit_idx][\"img_psth\"])  # (time, images)\n",
    "\n",
    "fig,axes = plt.subplots(2,1, figsize=(8,6), sharex=True)\n",
    "\n",
    "ax = axes[0]\n",
    "# normalize per image (shape-only)\n",
    "A_norm_time = A / (A.max(axis=0, keepdims=True) + 1e-9)\n",
    "sns.lineplot(A_norm_time, alpha=0.3, ax=ax)\n",
    "mean_trace = A_norm_time.mean(axis=1)\n",
    "ax.plot(np.arange(len(A)), mean_trace, color='k', lw=2.5)\n",
    "ax.set_title('per-image time course normalization')\n",
    "ax.legend().remove()\n",
    "\n",
    "ax = axes[1]\n",
    "# normalize per unit (all images)\n",
    "A_norm_unit = A / (A.max() + 1e-9)\n",
    "sns.lineplot(A_norm_unit, alpha=0.3, ax=ax)\n",
    "mean_trace = A_norm_unit.mean(axis=1)\n",
    "ax.plot(np.arange(len(A)), mean_trace, color='k', lw=2.5)\n",
    "ax.set_title('across-image time course normalization')\n",
    "ax.legend().remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start and end times calculated from CONFIG\n",
    "start = int(ONSET_MS / BIN_MS)\n",
    "end   = start + int(WINDOW_MS / BIN_MS)\n",
    "\n",
    "timecourses = []\n",
    "valid_idx = []  # track which rows succeed\n",
    "for i, row in dat.iterrows():\n",
    "    try:\n",
    "        tc = get_unit_timecourse(row, start, end)\n",
    "        timecourses.append(tc)\n",
    "        valid_idx.append(i)\n",
    "    except Exception as e:\n",
    "        # skip units that don't have valid PSTH\n",
    "        print(f\"Skipping unit {i}: {e}\")\n",
    "        pass\n",
    "    \n",
    "X = np.vstack(timecourses)  # (n_valid_units, T)\n",
    "\n",
    "# Z-score each unit across time (mean=0, std=1)\n",
    "X = zscore(X, axis=1)\n",
    "# Replace any NaNs from zero-variance rows with zeros\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# INERTIA/SILHOUETTE: choose K\n",
    "# =========================\n",
    "sil_scores = []\n",
    "labels_by_k = {}\n",
    "inertias = {}\n",
    "for k in tqdm(K_RANGE):\n",
    "    km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=N_INIT)\n",
    "    lab = km.fit_predict(X)\n",
    "    # inertia to measure fit of k clusters\n",
    "    km.fit(X)\n",
    "    inertias[k] = km.inertia_\n",
    "    labels_by_k[k] = lab\n",
    "    # alternatively, use silhouette\n",
    "    # requires >1 cluster and less than n_samples\n",
    "    sil = silhouette_score(X, lab)\n",
    "    sil_scores.append((k, sil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# by INERTIA\n",
    "df = pd.DataFrame({\n",
    "    \"k\": list(inertias.keys()),\n",
    "    \"inertia\": list(inertias.values())\n",
    "})\n",
    "\n",
    "cmap = sns.color_palette('husl')\n",
    "# seaborn lineplot\n",
    "fig,axes = plt.subplots(2,1, figsize=(5,5), sharex=True)\n",
    "ax = axes[0]\n",
    "sns.lineplot(data=df, x=\"k\", y=\"inertia\", marker=\"o\", linewidth=2.5, color='darkgrey', ax=ax)\n",
    "ax.set_title('Optimal cluster size')\n",
    "ax.set_ylabel('Inertia')\n",
    "ax.set_xlabel('')\n",
    "\n",
    "# by SILHOUETTE score\n",
    "# Pick best K\n",
    "best_k, best_sil = max(sil_scores, key=lambda t: t[1])\n",
    "labels = labels_by_k[best_k]\n",
    "\n",
    "print(\"Silhouette scores:\")\n",
    "sil_df = pd.DataFrame(columns=['k', 'silhouette_score'])\n",
    "for k, s in sil_scores:\n",
    "    print(f\"  k={k}: silhouette={s:.4f}\")\n",
    "    sil_df.loc[len(sil_df)] = {'k': k, 'silhouette_score': s}\n",
    "print(f\"=> Selected k={best_k} (silhouette={best_sil:.4f})\")\n",
    "\n",
    "ax = axes[1]\n",
    "sns.lineplot(sil_df, x='k', y='silhouette_score', marker=\"o\", linewidth=2.5, color='darkgrey', ax=ax)\n",
    "ax.set_xlabel('Cluster size (k)')\n",
    "ax.set_ylabel('Silhouette score')\n",
    "\n",
    "sns.despine(fig=fig, offset=5, trim=True)\n",
    "# plt.savefig('../../figs/optimal_k.png', transparent=True, dpi=300, bbox_inches='tight')\n",
    "# Attach labels back to the original `dat`\n",
    "# dat['cluster'] = np.nan\n",
    "dat.loc[valid_idx, 'cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# PLOTS: cluster mean time courses\n",
    "# =========================\n",
    "T = X.shape[1]\n",
    "time_axis = np.arange(start, end)  # time in ms\n",
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    \"time\": np.tile(time_axis, X.shape[0]),\n",
    "    \"response\": X.flatten(),\n",
    "    \"cluster\": np.repeat(labels, T)\n",
    "})\n",
    "\n",
    "# --- Seaborn plot ---\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,3))\n",
    "sns.lineplot(\n",
    "    data=df_plot,\n",
    "    x=\"time\",\n",
    "    y=\"response\",\n",
    "    hue=\"cluster\",\n",
    "    linewidth=1.5,\n",
    "    errorbar=(\"se\", 10), # for viz\n",
    "    palette=cmap,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# onset line\n",
    "ymin, ymax = ax.get_ylim()\n",
    "ax.vlines(x=50, ymin=ymin, ymax=ymax, color='red', linestyle='--')\n",
    "\n",
    "# plt.title(\"Clustered time courses\", fontsize=14)\n",
    "ax.set_xlabel(\"Time (msec)\")\n",
    "ax.set_ylabel(\"Response (z-scored)\")\n",
    "ax.legend(title=\"Cluster\", frameon=True, loc='lower right')\n",
    "ax.set_xlim(right=400)\n",
    "\n",
    "sns.despine(trim=True, offset=5)\n",
    "\n",
    "plt.savefig(os.path.join(SAVE_DIR, 'kmeans.png'), dpi=300, transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------- A) SUB-CLUSTER UNITS BY PSTH (within a cluster_id) ----------\n",
    "def subcluster_units_by_psth(\n",
    "    dat, cluster_id, onset_ms=50, window_ms=300, bin_ms=1,\n",
    "    k_range=range(2, 9), random_state=0, label_col_prefix=\"unit_subcluster\"\n",
    "):\n",
    "    sub = dat[dat[\"cluster\"] == cluster_id].copy()\n",
    "    if sub.empty:\n",
    "        raise ValueError(f\"No units for cluster_id={cluster_id}\")\n",
    "\n",
    "    start = onset_ms // bin_ms\n",
    "    end   = start + (window_ms // bin_ms)\n",
    "\n",
    "    rows, idx_keep = [], []\n",
    "    for i, r in sub.iterrows():\n",
    "        # prefer avg_psth; fallback to mean over images of img_psth\n",
    "        avg = get_unit_timecourse(r, start, end)\n",
    "        rows.append(avg)\n",
    "        idx_keep.append(i)\n",
    "\n",
    "    X = np.vstack(rows)                          # (n_units_in_cluster, T)\n",
    "    X = zscore(X, axis=1)\n",
    "    X = np.nan_to_num(X, 0.0)\n",
    "\n",
    "    # choose k via silhouette\n",
    "    best_k, best_score = max(\n",
    "        ((k, silhouette_score(X, KMeans(k, n_init=\"auto\", random_state=random_state).fit_predict(X)))\n",
    "         for k in k_range),\n",
    "        key=lambda t: t[1]\n",
    "    )\n",
    "\n",
    "    km = KMeans(best_k, n_init=\"auto\", random_state=random_state)\n",
    "    unit_labels = km.fit_predict(X)\n",
    "\n",
    "    # plot mean ± 95% CI per subcluster\n",
    "    T = X.shape[1]\n",
    "    time_axis = np.arange(start, end)\n",
    "    df_plot = pd.DataFrame({\n",
    "        \"time\": np.tile(time_axis, X.shape[0]),\n",
    "        \"resp\": X.ravel(),\n",
    "        \"subcluster\": np.repeat(unit_labels, T)\n",
    "    })\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.lineplot(data=df_plot, x=\"time\", y=\"resp\", hue=\"subcluster\",\n",
    "                 errorbar=(\"ci\", 95), lw=2)\n",
    "    plt.axvline(onset_ms, ls=\"--\", color=\"k\", alpha=0.5)\n",
    "    plt.title(f\"Cluster {cluster_id}: unit subclusters by PSTH (k={best_k})\")\n",
    "    plt.xlabel(\"Time (ms)\"); plt.ylabel(\"Z-scored response\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    return X, unit_labels, best_k\n",
    "\n",
    "# ------------------- HELPER FUNCTION --------------\n",
    "def z_cluster(dat, cluster_id, onset_ms=50, window_ms=300, bin_ms=1,\n",
    "              max_images=None, random_state=0, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Return z-scored PSTHs across units for one cluster:\n",
    "    Zu shape = (U, T, I_sel), z-scored over units at each (t, i).\n",
    "    \"\"\"\n",
    "    sub = dat[dat[\"cluster\"] == cluster_id]\n",
    "    if sub.empty:\n",
    "        raise ValueError(f\"No units for cluster_id={cluster_id}\")\n",
    "\n",
    "    # Stack to (U, T_full, I)\n",
    "    arrays = [np.asarray(x) for x in sub[\"img_psth\"].to_numpy()]\n",
    "    shapes = {a.shape for a in arrays}\n",
    "    if len(shapes) != 1:\n",
    "        raise ValueError(f\"Inconsistent img_psth shapes: {shapes}\")\n",
    "    X = np.stack(arrays, axis=0).astype(dtype, copy=False)\n",
    "    U, T_full, I = X.shape\n",
    "\n",
    "    # Window\n",
    "    start = onset_ms // bin_ms\n",
    "    end   = start + (window_ms // bin_ms)\n",
    "    if end > T_full:\n",
    "        raise ValueError(f\"window exceeds data: end={end}, T_full={T_full}\")\n",
    "\n",
    "    # Subsample images\n",
    "    if (max_images is not None) and (I > max_images):\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        img_sel = np.sort(rng.choice(I, size=max_images, replace=False))\n",
    "    else:\n",
    "        img_sel = np.arange(I)\n",
    "\n",
    "    X = X[:, start:end, :][:, :, img_sel]      # (U, T, I_sel)\n",
    "    # z-score across units at each (t, i)\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True) + 1e-8\n",
    "    Zu = (X - mu) / sd\n",
    "    return Zu  # (U, T, I_sel)\n",
    "\n",
    "# ---------- B) CLUSTER TIME POINTS BY REPRESENTATIONAL GEOMETRY ----------\n",
    "\n",
    "#     # per-time image×image RDM vectors (correlation distance)\n",
    "#     RDM_vecs = []\n",
    "#     for t in range(T):\n",
    "#         V = Zu[:, t, :].T                   # (I, U): images as observations, units as features\n",
    "#         rdm_vec = pdist(V, metric=\"correlation\")  # 1 - Pearson r\n",
    "#         RDM_vecs.append(rdm_vec)\n",
    "#     RDM_vecs = np.vstack(RDM_vecs)          # (T, P) [P = units * units-1 / 2]\n",
    "\n",
    "#     # normalize rows; cluster time points\n",
    "#     Xtime = (RDM_vecs - RDM_vecs.mean(axis=1, keepdims=True)) / (RDM_vecs.std(axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "#     best_k, best_score = max(\n",
    "#         ((k, silhouette_score(Xtime, KMeans(k, n_init=\"auto\", random_state=random_state).fit_predict(Xtime)))\n",
    "#          for k in k_range),\n",
    "#         key=lambda t: t[1]\n",
    "#     )\n",
    "\n",
    "#     km = KMeans(best_k, n_init=\"auto\", random_state=random_state)\n",
    "#     time_labels = km.fit_predict(Xtime)\n",
    "\n",
    "#     # simple strip view of time labels\n",
    "#     plt.figure(figsize=(9,1.6))\n",
    "#     sns.heatmap(time_labels[np.newaxis, :], cmap=\"tab10\", cbar=False)\n",
    "#     plt.title(f\"Cluster {cluster_id}: time clusters (k={best_k})\")\n",
    "#     plt.xlabel(\"Time (ms)\"); plt.yticks([])\n",
    "#     plt.tight_layout(); plt.show()\n",
    "\n",
    "#     rsa = None\n",
    "#     if return_rsa:\n",
    "#         # make time×time RSA for a sorted-block view\n",
    "#         X0 = Xtime - Xtime.mean(axis=1, keepdims=True)\n",
    "#         X0 /= (np.linalg.norm(X0, axis=1, keepdims=True) + 1e-12)\n",
    "#         rsa = X0 @ X0.T                       # (T, T)\n",
    "\n",
    "#         order = np.argsort(time_labels)\n",
    "#         rsa_sorted = rsa[order][:, order]\n",
    "#         plt.figure(figsize=(6,5))\n",
    "#         sns.heatmap(rsa_sorted, vmin=0, vmax=1, cmap=\"Spectral_r\")\n",
    "#         plt.title(f\"Time×Time RSA (Cluster {cluster_id}, reordered by time clusters)\")\n",
    "#         plt.tight_layout(); plt.show()\n",
    "\n",
    "#     return time_labels, best_k, RDM_vecs, rsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cluster_id in range(3):\n",
    "    # A) sub-cluster units by PSTH\n",
    "    X, unit_labels, best_k_units = subcluster_units_by_psth(dat, cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_IMAGES = 200\n",
    "RNG_SEED = 0\n",
    "cluster_id = 1\n",
    "\n",
    "# --- SELECT UNITS FOR THIS CLUSTER ---\n",
    "sub = dat[dat[\"cluster\"] == cluster_id]\n",
    "if sub.empty:\n",
    "    raise ValueError(f\"No units for cluster_id={cluster_id}\")\n",
    "\n",
    "# Keep arrays as a Python list to avoid building a big 3D tensor\n",
    "arrays = [np.asarray(x, dtype=np.float32) for x in sub[\"img_psth\"].to_numpy()]\n",
    "shapes = {a.shape for a in arrays}\n",
    "if len(shapes) != 1:\n",
    "    raise ValueError(f\"Inconsistent img_psth shapes: {shapes}\")\n",
    "T_full, I_total = arrays[0].shape\n",
    "U = len(arrays)\n",
    "\n",
    "# --- WINDOW IN TIME ---\n",
    "start = 0\n",
    "end   = 350\n",
    "if end > T_full:\n",
    "    raise ValueError(f\"window exceeds data (end={end} > T_full={T_full})\")\n",
    "T = end - start\n",
    "\n",
    "# --- (Optional) SUBSAMPLE IMAGES FOR SPEED ---\n",
    "if (MAX_IMAGES is not None) and (I_total > MAX_IMAGES):\n",
    "    rng = np.random.default_rng(RNG_SEED)\n",
    "    img_sel = np.sort(rng.choice(I_total, size=MAX_IMAGES, replace=False))\n",
    "else:\n",
    "    img_sel = np.arange(I_total)\n",
    "I_sel = len(img_sel)\n",
    "print(img_sel.shape)\n",
    "# --- PREP OUTPUT (condensed RDM per time) ---\n",
    "P = I_sel * (I_sel - 1) // 2\n",
    "RDM_vecs = np.empty((T, P), dtype=np.float32)\n",
    "\n",
    "# --- STREAM OVER TIME (NO BIG Zu) ---\n",
    "for ti, t in tqdm(enumerate(range(start, end))):\n",
    "    # Assemble V: units × images at this time\n",
    "    V = np.empty((U, I_sel), dtype=np.float32)\n",
    "    for u, A in enumerate(arrays):\n",
    "        V[u, :] = A[t, img_sel]\n",
    "\n",
    "    # Z-score across units per image (so correlations reflect population pattern)\n",
    "    V -= V.mean(axis=0, keepdims=True)\n",
    "    sd = V.std(axis=0, keepdims=True)\n",
    "    sd[sd < 1e-8] = 1.0\n",
    "    V /= sd\n",
    "\n",
    "    # RDM for this time: 1 - Pearson r between images (observations), features = units\n",
    "    rdm_vec = pdist(V.T, metric=\"correlation\")   # condensed upper triangle\n",
    "    RDM_vecs[ti, :] = rdm_vec.astype(np.float32)\n",
    "\n",
    "print(\"RDM_vecs:\", RDM_vecs.shape)  # (T, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- TIME×TIME RSA (correlate RDM vectors across times) ---\n",
    "X = RDM_vecs - RDM_vecs.mean(axis=1, keepdims=True)\n",
    "X /= (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "rsa = X @ X.T   # (T, T) in [-1, 1]\n",
    "\n",
    "# --- PLOT RSA WITH CONTOURS (0.3, 0.5) ---\n",
    "plt.figure(figsize=(6,5))\n",
    "ax = plt.imshow(rsa, vmin=0, vmax=1, origin = 'upper', cmap=\"Spectral_r\")\n",
    "plt.colorbar()\n",
    "levels = [0.3, 0.5]\n",
    "xgrid = np.arange(T)\n",
    "ygrid = np.arange(T)\n",
    "CS = plt.contour(xgrid, ygrid, rsa, levels=levels, colors='gray', linewidths=0.8)\n",
    "plt.clabel(CS, fmt=\"%.1f\", colors='gray', fontsize=8)\n",
    "plt.title(f\"Time×Time RSA — cluster {cluster_id} (images used: {I_sel})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import BisectingKMeans\n",
    "\n",
    "model = BisectingKMeans(n_clusters=8, random_state=0)\n",
    "labels = model.fit_predict(X)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Xtime : (T, vRDM [maxi * maxi-1 / 2])  – same as used for K-means\n",
    "# you can use the RDM vectors or your average PSTH vectors\n",
    "X = Xtime\n",
    "\n",
    "# build hierarchical linkage tree (using scipy)\n",
    "Z = linkage(X, method=\"ward\")  # or 'average', 'complete', etc.\n",
    "\n",
    "# dendrogram visualization\n",
    "plt.figure(figsize=(10, 4))\n",
    "dendrogram(Z, no_labels=True, color_threshold=0)\n",
    "plt.title(\"Hierarchical clustering of timepoints\")\n",
    "plt.xlabel(\"Time index\")\n",
    "plt.ylabel(\"Linkage distance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# choose number of clusters (like cutting the tree)\n",
    "hclust = AgglomerativeClustering(n_clusters=5, linkage=\"ward\")\n",
    "labels_hier = hclust.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def bisecting_tree(X, target_k=8, random_state=0):\n",
    "    clusters = {0: np.arange(len(X))}   # cluster_id -> sample indices\n",
    "    next_id = 1\n",
    "\n",
    "    while len(clusters) < target_k:\n",
    "        # pick cluster with largest SSE\n",
    "        sse = {cid: np.sum((X[idx] - X[idx].mean(axis=0))**2)\n",
    "               for cid, idx in clusters.items()}\n",
    "        cid_split = max(sse, key=sse.get)\n",
    "        idx = clusters.pop(cid_split)\n",
    "\n",
    "        km = KMeans(n_clusters=2, random_state=random_state).fit(X[idx])\n",
    "        lbls = km.labels_\n",
    "        clusters[next_id]   = idx[lbls == 0]\n",
    "        clusters[next_id+1] = idx[lbls == 1]\n",
    "        next_id += 2\n",
    "    return clusters\n",
    "\n",
    "clusters = bisecting_tree(X)\n",
    "\n",
    "n_samples = max(int(i.max()) for i in clusters.values()) + 1\n",
    "labels = np.full(n_samples, -1, dtype=int)\n",
    "for cid, idx in clusters.items():\n",
    "    labels[idx] = cid\n",
    "\n",
    "plt.figure(figsize=(10, 1.5))\n",
    "sns.heatmap(labels[np.newaxis, :], cmap=\"tab20\", cbar=False)\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Time (bins or ms)\")\n",
    "plt.title(\"Bisecting K-means — final cluster membership\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# RSA per cluster (time × time similarity)\n",
    "# =========================\n",
    "# For RSA, we use img_psth (time x images) for units in each cluster.\n",
    "# At each time bin within the window, build population-by-image matrix for that cluster:\n",
    "#   M_t = (n_units_in_cluster x n_images_sub)\n",
    "# Compute image×image correlation matrix at that time => similarity\n",
    "# Convert to RDM vector = 1 - corr (upper triangle).\n",
    "# Compare RDM vectors across all time pairs via correlation => time×time RSA.\n",
    "\n",
    "def rdm_vector_from_matrix(M, metric='correlation'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        M: (ndarray)\n",
    "        metric: (str)\n",
    "    Given a matrix M with shape (n_units, n_images), compute the image×image\n",
    "    RDM and return the vectorized upper triangle (excluding diagonal).\n",
    "    \"\"\"\n",
    "    M = np.asarray(M, dtype=float)\n",
    "    # center columns (images) across units, matching the corrcoef approach\n",
    "    M -= M.mean(axis=0, keepdims=True)\n",
    "    M = np.nan_to_num(M, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    # pdist expects observations in rows, so pass images as rows -> M.T\n",
    "    rdm_vec = pdist(M.T, metric=metric)  # already 1 - corr\n",
    "    return rdm_vec\n",
    "\n",
    "def rsa_time_by_time_for_cluster(cluster_id, metric='correlation', vectorized=True):\n",
    "    idx_units = np.where(dat.loc[valid_idx, 'cluster'].values == cluster_id)[0]\n",
    "\n",
    "    # Select only the rows you want, based on cluster id\n",
    "    subset = dat.iloc[idx_units][\"img_psth\"]\n",
    "\n",
    "    # Convert to list of arrays\n",
    "    arrays = [np.asarray(x) for x in subset]\n",
    "    # shape: (units, time_points, images)\n",
    "    img_psth_array = np.stack(arrays, axis=0)\n",
    "    Z = img_psth_array[:, start:end, :]\n",
    "    U, T, I = Z.shape\n",
    "\n",
    "    # 1) z-score across units for each (t, i)\n",
    "    mu = Z.mean(axis=0, keepdims=True)                  # (1, T, I)\n",
    "    sd = Z.std(axis=0, keepdims=True) + 1e-8           # (1, T, I)\n",
    "    Zu = (Z - mu) / sd                                  # (U, T, I)\n",
    "\n",
    "    # 2) per-time image×image correlation matrices (population-based)\n",
    "    #    For each t: C_t = (Zu_t^T @ Zu_t) / (U-1)\n",
    "\n",
    "\n",
    "    vectorized = True\n",
    "    if vectorized:\n",
    "        C_time = np.einsum('uti,utj->tij', Zu, Zu) / (U - 1)   # (T, I, I)\n",
    "        # C_time = np.clip(C_time, -1.0, 1.0)\n",
    "    else:\n",
    "        C_time = Zu.map(lambda A: pdist(np.asarray(A).T, metric=metric))\n",
    "        \n",
    "    RDM_time = 1.0 - C_time                                # (T, I, I)\n",
    "    # 3) vectorize upper triangles to get one RDM vector per time\n",
    "    iu = np.triu_indices(I, k=1)\n",
    "    RDM_vecs = RDM_time[:, iu[0], iu[1]]                   # (T, P), P = I*(I-1)/2\n",
    "\n",
    "    X = RDM_vecs - RDM_vecs.mean(axis=1, keepdims=True)     # demean per time\n",
    "    den = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12\n",
    "    Xn = X / den\n",
    "    rsa = Xn @ Xn.T   \n",
    "    \n",
    "    return rsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range=range(2, 11)\n",
    "Xtime = (RDM_vecs - RDM_vecs.mean(axis=1, keepdims=True)) / (RDM_vecs.std(axis=1, keepdims=True) + 1e-9)\n",
    "best_k, best_score = max(\n",
    "    ((k, silhouette_score(Xtime, KMeans(k, n_init=\"auto\", random_state=RANDOM_STATE).fit_predict(Xtime)))\n",
    "     for k in k_range),\n",
    "    key=lambda t: t[1]\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(3,3, figsize=(20,20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, k in enumerate(k_range):\n",
    "    ax = axes[idx]\n",
    "    # bisecting Kmeans makes groups hierarchically\n",
    "    # km = KMeans(best_k, n_init=\"auto\", random_state=random_state)\n",
    "    # km = BisectingKMeans(best_k, random_state=random_state)\n",
    "    # km = KMeans(k, n_init=\"auto\", random_state=random_state)\n",
    "    km = BisectingKMeans(k, random_state=random_state)\n",
    "    \n",
    "    time_labels = km.fit_predict(Xtime)\n",
    "    \n",
    "    # simple strip view of time labels\n",
    "    # plt.figure(figsize=(9,1.6))\n",
    "    # sns.heatmap(time_labels[np.newaxis, :], cmap=\"tab10\", cbar=False)\n",
    "    # plt.title(f\"Cluster {cluster_id}: time clusters (k={best_k})\")\n",
    "    # plt.xlabel(\"Time (ms)\"); plt.yticks([])\n",
    "    # plt.tight_layout(); plt.show()\n",
    "    \n",
    "    X0 = Xtime - Xtime.mean(axis=1, keepdims=True)\n",
    "    X0 /= (np.linalg.norm(X0, axis=1, keepdims=True) + 1e-12)\n",
    "    rsa = X0 @ X0.T                       # (T, T)\n",
    "    \n",
    "    order = np.argsort(time_labels, kind=\"stable\")\n",
    "    time_axis_ms = ONSET_MS + np.arange(T)*BIN_MS\n",
    "    labels_sorted = time_labels[order]\n",
    "    sorted_times_ms = time_axis_ms[order]\n",
    "    \n",
    "    rsa_sorted = rsa\n",
    "    rsa_sorted = rsa[order][:, order]\n",
    "    \n",
    "    sorted_labels = time_labels[order]\n",
    "    sorted_times  = time_axis_ms[order]   # original times (ms) after sorting\n",
    "    \n",
    "    # find where cluster label changes\n",
    "    boundaries = np.where(np.diff(sorted_labels) != 0)[0] + 1   # indices after a change\n",
    "    tick_idx   = np.concatenate(([0], boundaries, [len(sorted_labels)]))\n",
    "    \n",
    "    # for each boundary region, use the *first* time label as tick\n",
    "    tick_labels = [f\"{int(sorted_times[i])}\" for i in tick_idx[:-1]]  # omit last edge\n",
    "    \n",
    "    sns.heatmap(rsa_sorted, vmin=0, vmax=1, cmap=\"Spectral_r\", cbar=False, ax=ax)\n",
    "    \n",
    "    # adjust ticks to cell centers (+0.5 offset)\n",
    "    ax.set_xticks(tick_idx[:-1] + 0.5)\n",
    "    ax.set_yticks(tick_idx[:-1] + 0.5)\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "    \n",
    "    # optional: draw boundary lines\n",
    "    for b in boundaries:\n",
    "        ax.axhline(b, color=\"k\", lw=0.6, alpha=0.5)\n",
    "        ax.axvline(b, color=\"k\", lw=0.6, alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel(\"Block start time (ms, original)\")\n",
    "    ax.set_ylabel(\"Block start time (ms, original)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(X, metric='euclidean', method='ward', cmap='vlag', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rsa_results = {}\n",
    "for c in tqdm(range(best_k)):\n",
    "    print(f\"[RSA] Computing time×time similarity for cluster {c} ...\")\n",
    "    out = rsa_time_by_time_for_cluster(c)\n",
    "    if out is None:\n",
    "        continue\n",
    "    rsa_mat = out\n",
    "    rsa_results[c] = rsa_mat\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(rsa_mat, origin='lower', aspect='auto', vmin=0, vmax=1,\n",
    "              cmap='Spectral')\n",
    "    plt.colorbar(label=\"RSA (RDM–RDM correlation)\")\n",
    "\n",
    "    # add contour lines at correlation levels 0.3 and 0.5\n",
    "    levels = [0.3, 0.5]\n",
    "    contours = plt.contour(\n",
    "        np.linspace(start, end-1, rsa_mat.shape[0]),  # x-axis\n",
    "        np.linspace(start, end-1, rsa_mat.shape[1]),  # y-axis\n",
    "        rsa_mat,\n",
    "        levels=levels,\n",
    "        colors='gray',\n",
    "        linewidths=0.8,\n",
    "    )\n",
    "    plt.clabel(contours, fmt=\"%.1f\", colors='gray', fontsize=8)\n",
    "\n",
    "    plt.title(f\"Time×Time RSA — Cluster {c}\")\n",
    "    plt.xlabel(\"Time (ms)\")\n",
    "    plt.ylabel(\"Time (ms)\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_id = 0\n",
    "\n",
    "idx_units = np.where(dat.loc[valid_idx, 'cluster'].values == cluster_id)[0]\n",
    "if len(idx_units) < 2:\n",
    "    print(f\"[RSA] Cluster {cluster_id}: not enough units ({len(idx_units)})\")\n",
    "\n",
    "# Stack img_psth for units in this cluster\n",
    "# We may subsample images for speed/memory\n",
    "# First, figure out number of images from the first unit\n",
    "sample_unit = dat.iloc[valid_idx[idx_units[0]]]['img_psth']\n",
    "A0 = np.asarray(sample_unit)\n",
    "if A0.ndim != 2:\n",
    "    raise ValueError(\"img_psth must be 2D (time x images)\")\n",
    "n_time, n_images = A0.shape\n",
    "print(A0.shape)\n",
    "\n",
    "# Subsample images if needed\n",
    "if MAX_IMAGES_FOR_RSA is not None and n_images > MAX_IMAGES_FOR_RSA:\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    img_sel = np.sort(rng.choice(n_images, size=MAX_IMAGES_FOR_RSA, replace=False))\n",
    "else:\n",
    "    img_sel = np.arange(n_images)\n",
    "print(img_sel.shape)\n",
    "\n",
    "# # Prepare an array: for each time t in [start:end), we will build an RDM vector\n",
    "# rdm_vecs = []\n",
    "# for t in tqdm(range(50, 55)):\n",
    "#     # Build matrix (units x images_sel) at this time\n",
    "#     rows = []\n",
    "#     for u_rel in idx_units:\n",
    "#         unit_idx = valid_idx[u_rel]\n",
    "#         A = np.asarray(dat.iloc[unit_idx]['img_psth'])  # (time, images)\n",
    "#         rows.append(A[t, img_sel])\n",
    "#     M = np.vstack(rows)  # (n_units_in_cluster, n_images_sub)\n",
    "#     # Normalize across units for stability (optional)\n",
    "#     # M = zscore(M, axis=0)  # z-score per image across units\n",
    "#     M = np.nan_to_num(M, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "#     # upper triangular of image x image RDMS\n",
    "#     triu = rdm_vector_from_matrix(M)\n",
    "#     rdm_vecs.append(triu)\n",
    "\n",
    "# rdm_vecs = np.vstack(rdm_vecs)  # (T, n_pairs)\n",
    "# print(rdm_vecs.shape)\n",
    "# # time x time similarity by correlating RDM vectors across time\n",
    "# Tloc = rdm_vecs.shape[0]\n",
    "# rsa_mat = np.empty((Tloc, Tloc), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "RDM of average time traces, by cluster\n",
    "'''\n",
    "cluster_id = 1\n",
    "\n",
    "idx_units = np.where(dat.loc[valid_idx, 'cluster'].values == cluster_id)[0]\n",
    "\n",
    "# Select only the rows you want, based on cluster id\n",
    "subset = dat.iloc[idx_units][\"img_psth\"]\n",
    "\n",
    "# Convert to list of arrays\n",
    "arrays = [np.asarray(x) for x in subset]\n",
    "# shape: (units, time_points, images)\n",
    "img_psth_array = np.stack(arrays, axis=0)\n",
    "\n",
    "# average across images to get each unit's average psth\n",
    "avg_psth = np.mean(img_psth_array, axis=2)\n",
    "# normalize each timecourse\n",
    "avg_psth_norm = zscore(avg_psth, axis=1)\n",
    "# subset to specific time range\n",
    "avg_psth_norm = avg_psth_norm[:, :350]\n",
    "\n",
    "# rdm\n",
    "metric = 'correlation'\n",
    "rdm_vec = pdist(avg_psth_norm.T, metric=metric)\n",
    "rdm = squareform(rdm_vec)\n",
    "sns.heatmap(rdm)\n",
    "plt.title(f'Cluster {cluster_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "metric = 'correlation'\n",
    "tmax = 350\n",
    "\n",
    "dsub = dat.loc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "n_pcs = 300 # math.floor(min(dsub['cluster'].value_counts()) / 1000) * 1000\n",
    "\n",
    "clusters = sorted(pd.unique(dsub['cluster']))\n",
    "K = len(clusters)\n",
    "\n",
    "# store time x pc representations per cluster\n",
    "time_pcs = {}\n",
    "\n",
    "for cid in tqdm(clusters):\n",
    "    idx_units = np.where(dsub['cluster'].values == cid)[0]\n",
    "    subset = dsub.iloc[idx_units]['img_psth']\n",
    "\n",
    "    arrays = [np.asarray(x) for x in subset]\n",
    "    img_psth = np.stack(arrays, axis=0)        # [units, time, images]\n",
    "\n",
    "    avg_psth = img_psth.mean(axis=2)            # [units, time]\n",
    "    avg_psth = zscore(avg_psth, axis=1)         # normalize per unit\n",
    "    avg_psth = avg_psth[:, :tmax]\n",
    "\n",
    "    # PCA over units\n",
    "    pca = PCA(n_components=min(n_pcs, avg_psth.shape[0]), svd_solver='full')\n",
    "    pcs = pca.fit_transform(avg_psth.T)         # [time, n_pcs]\n",
    "\n",
    "    time_pcs[cid] = pcs                         # store time representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot KxK grid of heatmaps\n",
    "fig, axes = plt.subplots(K, K, figsize=(3.2 * K, 3.0 * K), squeeze=False)\n",
    "\n",
    "# correlation distance lives in [0, 2]; fix scale so panels are comparable\n",
    "vmin, vmax = 0.0, 2.0\n",
    "\n",
    "for i, ci in enumerate(clusters):\n",
    "    for j, cj in enumerate(clusters):\n",
    "        ax = axes[i, j]\n",
    "\n",
    "        if i == j:\n",
    "            # within-cluster: time x time rdm\n",
    "            rdm = squareform(pdist(time_pcs[ci], metric=metric))   # [tmax, tmax]\n",
    "            title = f'cluster {int(ci)} vs {int(ci)}'\n",
    "        else:\n",
    "            # cross-cluster: time (ci) x time (cj) rdm\n",
    "            rdm = cdist(time_pcs[ci], time_pcs[cj], metric=metric) # [tmax, tmax]\n",
    "            title = f'cluster {int(ci)} vs {int(cj)}'\n",
    "\n",
    "        sns.heatmap(\n",
    "            rdm,\n",
    "            ax=ax,\n",
    "            square=True,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            cbar=False,\n",
    "        )\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('time')\n",
    "        ax.set_ylabel('time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how the paper does it??\n",
    "cluster_id = 2\n",
    "num_I = 50\n",
    "random_state = 0\n",
    "\n",
    "idx_units = np.where(dat.loc[valid_idx, 'cluster'].values == cluster_id)[0]\n",
    "\n",
    "# Select only the rows you want, based on cluster id\n",
    "subset = dat.iloc[idx_units][\"img_psth\"]\n",
    "\n",
    "# Convert to list of arrays\n",
    "arrays = [np.asarray(x) for x in subset]\n",
    "# shape: (units, time_points, images)\n",
    "img_psth_array = np.stack(arrays, axis=0)\n",
    "\n",
    "print(len(img_psth_array))\n",
    "\n",
    "# subset the images\n",
    "rng = np.random.default_rng(random_state)\n",
    "img_sel = np.sort(rng.choice(img_psth_array.shape[2], size=num_I, replace=False))\n",
    "\n",
    "Z = img_psth_array[:, start:end, :][:, :, img_sel]\n",
    "U, T, I = Z.shape\n",
    "\n",
    "# 1) z-score across units for each (t, i)\n",
    "mu = Z.mean(axis=0, keepdims=True)                  # (1, T, I)\n",
    "sd = Z.std(axis=0, keepdims=True) + 1e-8           # (1, T, I)\n",
    "Zu = (Z - mu) / sd                                  # (U, T, I)\n",
    "\n",
    "# 2) per-time image×image correlation matrices (population-based)\n",
    "#    For each t: C_t = (Zu_t^T @ Zu_t) / (U-1)\n",
    "\n",
    "\n",
    "vectorized = True\n",
    "if vectorized:\n",
    "    C_time = np.einsum('uti,utj->tij', Zu, Zu) / (U - 1)   # (T, I, I)\n",
    "    # C_time = np.clip(C_time, -1.0, 1.0)\n",
    "else:\n",
    "    for unit in Z:\n",
    "        print(unit.shape)\n",
    "        break\n",
    "\n",
    "print(f'this shape should be time x img x img', C_time.shape)\n",
    "# # average across images to get each unit's average psth\n",
    "# avg_psth = np.mean(img_psth_array, axis=2)\n",
    "# # normalize each timecourse\n",
    "# avg_psth_norm = zscore(avg_psth, axis=1)\n",
    "# # subset to specific time range\n",
    "# avg_psth_norm = avg_psth_norm[:, :350]\n",
    "\n",
    "# # rdm\n",
    "# metric = 'correlation'\n",
    "# rdm_vec = pdist(avg_psth_norm.T, metric=metric)\n",
    "# rdm = squareform(rdm_vec)\n",
    "# sns.heatmap(rdm)\n",
    "# plt.title(f'Cluster {cluster_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RDM_time = 1.0 - C_time                                # (T, I, I)\n",
    "\n",
    "# 3) vectorize upper triangles to get one RDM vector per time\n",
    "iu = np.triu_indices(I, k=1)\n",
    "RDM_vecs = RDM_time[:, iu[0], iu[1]]                   # (T, P), P = I*(I-1)/2\n",
    "\n",
    "X = RDM_vecs - RDM_vecs.mean(axis=1, keepdims=True)     # demean per time\n",
    "den = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12\n",
    "Xn = X / den\n",
    "rsa = Xn @ Xn.T   \n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "ax = plt.imshow(rsa, vmin=0, vmax=1, origin='lower', cmap=\"Spectral_r\")\n",
    "plt.colorbar()\n",
    "\n",
    "levels = [0.3, 0.5]\n",
    "Xgrid = np.linspace(0, T-1, T)\n",
    "Ygrid = np.linspace(0, T-1, T)\n",
    "CS = plt.contour(Xgrid, Ygrid, rsa, levels=levels, colors='gray', linewidths=0.8)\n",
    "plt.clabel(CS, fmt=\"%.1f\", colors='gray', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "10:35 --> in the middle of this\n",
    "Z should be a unit-normalized psth array with size (unit, timepoint, img)\n",
    "i want to create img x img RDMs, separately for each unit\n",
    "this is supposed to be a vectorized version, but it takes forever\n",
    "\n",
    "ultimately, i need to do pairwise comparisons of the RDM vector for each pair of timepoints\n",
    "\n",
    "so t1, t2 --> corr(rdv_t1, rdv_t2)\n",
    "rdv_t1 --> image x image RDM, from unit x image matrix?\n",
    "yeah i think this ^^\n",
    "'''\n",
    "vectorized = False\n",
    "unit_max = subset.max(axis=(1, 2), keepdims=True)\n",
    "Z = subset / (unit_max + 1e-9)\n",
    "\n",
    "if vectorized:\n",
    "    C = np.einsum('uti,utj->uij', Z, Z) / (T - 1) \n",
    "    C.shape\n",
    "else:\n",
    "    for unit in Z:\n",
    "        print(unit.shape)\n",
    "        break\n",
    "\n",
    "#     # subset: Series where each element is a (time, images) array for one unit\n",
    "#     metric = \"correlation\"\n",
    "\n",
    "#     # apply pdist to each unit's matrix\n",
    "#     rdm_list = subset.map(lambda A: pdist(np.asarray(A).T, metric=metric))\n",
    "\n",
    "#     # optional: convert to array of shape (n_units, n_pairs)\n",
    "#     rdm_array = np.stack(rdm_list.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG (edit as needed)\n",
    "# =========================\n",
    "TYPE2_CLUSTER = 1          # set to the cluster label that corresponds to \"Type 2\"\n",
    "MONKEY_ID = 3\n",
    "ONSET_MS = 0              # stimulus onset (ms)\n",
    "WINDOW_MS = 350            # analyze 0–300 ms post-onset\n",
    "BIN_MS = 1                 # ms/bin (change if your PSTH bin differs)\n",
    "THRESH_NORM_AVG = 0.3      # drop images below this normalized average response\n",
    "N_SHOW = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_anterior_face_roi(roi):\n",
    "    if roi is None:\n",
    "        return False\n",
    "    tok0 = str(roi).split(\"_\", 1)[0]  # first chunk only\n",
    "    return \"a\" in tok0.lower()\n",
    "\n",
    "mask = (\n",
    "    (dat[\"monkey\"] == MONKEY_ID) &\n",
    "    (dat[\"cluster\"] == TYPE2_CLUSTER) &\n",
    "    (dat[\"roi\"].apply(is_anterior_face_roi))\n",
    ")\n",
    "\n",
    "subset = dat.loc[mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BUILD POPULATION PSTH PER IMAGE\n",
    "# Per-unit scalar normalization = divide each unit by its max within [onset : onset+300ms] across ALL images.\n",
    "# Then average normalized traces across units to get population PSTH per image.\n",
    "# =========================\n",
    "start = int(ONSET_MS / BIN_MS)\n",
    "end   = start + int(WINDOW_MS / BIN_MS)\n",
    "\n",
    "# infer sizes from first valid unit\n",
    "sample = np.asarray(subset.iloc[0][\"img_psth\"])   # (time, images), e.g., (450, 1072)\n",
    "T_full, N_img = sample.shape\n",
    "\n",
    "pop_psth = np.zeros((end - start, N_img), dtype=float)\n",
    "n_units = 0\n",
    "\n",
    "for _, row in subset.iterrows():\n",
    "    A = np.asarray(row[\"img_psth\"])\n",
    "    if A.ndim != 2 or A.shape[0] < end:\n",
    "        continue\n",
    "    W = A[start:end, :]          # (Twin, N_img)\n",
    "    u_max = W.max()              # scalar per unit\n",
    "    if not np.isfinite(u_max) or u_max <= 0:\n",
    "        continue\n",
    "    pop_psth += (W / u_max)\n",
    "    n_units += 1\n",
    "\n",
    "if n_units == 0:\n",
    "    raise RuntimeError(\"No units passed windowing/normalization.\")\n",
    "pop_psth /= n_units  # average across units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(norm_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# IMAGE-WISE METRICS\n",
    "# =========================\n",
    "norm_avg   = pop_psth.mean(axis=0)       # mean over time\n",
    "peak_amp   = pop_psth.max(axis=0)        # max over time (y-axis)\n",
    "peak_idx   = pop_psth.argmax(axis=0)     # index of max\n",
    "peak_lat_ms= peak_idx * BIN_MS           # ms since onset (x-axis)\n",
    "\n",
    "# Apply exclusion: normalized average < 0.7\n",
    "THRESH_NORM_AVG = 0\n",
    "keep = norm_avg >= THRESH_NORM_AVG\n",
    "kept_count = int(keep.sum())\n",
    "if kept_count == 0:\n",
    "    raise RuntimeError(\"No images passed THRESH_NORM_AVG; consider lowering it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SCATTER: Peak latency (x) vs normalized peak firing (y)\n",
    "# =========================\n",
    "plt.figure(figsize=(6.5, 5))\n",
    "plt.scatter(peak_lat_ms[keep], peak_amp[keep], s=12, alpha=0.65, edgecolor=\"none\")\n",
    "plt.xlabel(\"Peak latency (ms since onset)\")\n",
    "plt.ylabel(\"Normalized peak firing (a.u.)\")\n",
    "plt.title(f\"Type 2 · M3 anterior face area · {kept_count} images\")\n",
    "plt.grid(alpha=0.25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FULL TIME TRACES for top-N images (by normalized peak)\n",
    "# Use the SAME per-unit scalar normalization (from 0–300 ms window across all images),\n",
    "# but now plot the FULL 0..T_full trajectory.\n",
    "# =========================\n",
    "top_idx_within_kept = np.argsort(peak_amp[keep])[-N_SHOW:]          # positions within kept subset\n",
    "kept_image_ids = np.where(keep)[0]                                   # original image indices\n",
    "top_image_ids = kept_image_ids[top_idx_within_kept][::-1]            # top N, descending\n",
    "\n",
    "time_axis_full = np.arange(T_full) * BIN_MS\n",
    "pop_traces_full = []  # averaged across units per image\n",
    "\n",
    "for img in top_image_ids:\n",
    "    acc = np.zeros(T_full, dtype=float)\n",
    "    cnt = 0\n",
    "    for _, row in subset.iterrows():\n",
    "        A = np.asarray(row[\"img_psth\"])  # (T_full, N_img)\n",
    "        if A.ndim != 2 or img >= A.shape[1] or A.shape[0] < end:\n",
    "            continue\n",
    "        # reuse same scalar normalization\n",
    "        u_max = A[start:end, :].max()\n",
    "        if not np.isfinite(u_max) or u_max <= 0:\n",
    "            continue\n",
    "        acc += (A[:, img] / u_max)\n",
    "        cnt += 1\n",
    "    if cnt > 0:\n",
    "        pop_traces_full.append(acc / cnt)\n",
    "    else:\n",
    "        pop_traces_full.append(None)\n",
    "\n",
    "plt.figure(figsize=(7.5, 5))\n",
    "for img, tr in zip(top_image_ids, pop_traces_full):\n",
    "    if tr is None:\n",
    "        continue\n",
    "    lbl = f\"img {int(img)} (peak {int(peak_lat_ms[img])} ms)\"\n",
    "    plt.plot(time_axis_full, tr, lw=1.6, alpha=0.9, label=lbl)\n",
    "plt.axvline(ONSET_MS, ls=\"--\", lw=1, color=\"k\", alpha=0.6)\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(\"Normalized population response (a.u.)\")\n",
    "plt.title(f\"Top {N_SHOW} images by normalized peak · Type 2 · M3 AF\")\n",
    "plt.legend(fontsize=8, frameon=False)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv-py3 (manifold-dynamics)",
   "language": "python",
   "name": "manifold-dynamics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
