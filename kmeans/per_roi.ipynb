{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import zscore, pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_pickle('../../datasets/NNN/unit_data_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['roi'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "ONSET_MS = 0          # stimulus onset (ms)\n",
    "WINDOW_MS = 350        # analyze 0-300 ms post onset\n",
    "BIN_MS = 1             # temporal bin size (ms). If not 1, change indices below\n",
    "K_RANGE = range(2, 11) # candidates for number of clusters\n",
    "RANDOM_STATE = 0\n",
    "N_INIT = \"auto\"        # or an int (e.g., 20) if on older sklearn\n",
    "MAX_IMAGES_FOR_RSA = None   # to keep RSA runtime reasonable. None for all images\n",
    "RSA_METRIC = \"correlation\"       # correlation type when comparing RDM vectors\n",
    "SAVE_PREFIX = \"clustering_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unit_timecourse(row, img_start=None, img_end=None, t_start=None, t_end=None):\n",
    "    \"\"\"\n",
    "    Return a 1D timecourse for a unit, optionally sliced by image and time indices.\n",
    "\n",
    "    Behavior:\n",
    "    - If an image slice is requested (img_start or img_end is not None), compute the\n",
    "      average over that image range from row['img_psth'] (shape: time x images).\n",
    "    - Otherwise, prefer row['avg_psth'] if present; if absent, average across ALL images\n",
    "      from row['img_psth'].\n",
    "    - If t_start/t_end are provided, return avg[t_start:t_end]. If neither is provided,\n",
    "      return the full available timecourse.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pandas.Series\n",
    "        Must contain 'img_psth' (time x images). Optionally 'avg_psth' (time,).\n",
    "    img_start, img_end : int or None\n",
    "        Image index slice [img_start:img_end]. None means open-ended (use all).\n",
    "    t_start, t_end : int or None\n",
    "        Time index slice [t_start:t_end]. None means open-ended (use all).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, shape (T,)\n",
    "        1D timecourse. If both t_start and t_end are provided, T == (t_end - t_start).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If shapes are invalid, required fields are missing, or indices are out of range.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Coerce indices to ints if provided\n",
    "    def _to_int_or_none(x, name):\n",
    "        if x is None:\n",
    "            return None\n",
    "        try:\n",
    "            return int(x)\n",
    "        except Exception:\n",
    "            raise ValueError(f\"{name} must be an int or None (got {x!r})\")\n",
    "\n",
    "    img_start = _to_int_or_none(img_start, \"img_start\")\n",
    "    img_end   = _to_int_or_none(img_end,   \"img_end\")\n",
    "    t_start   = _to_int_or_none(t_start,   \"t_start\")\n",
    "    t_end     = _to_int_or_none(t_end,     \"t_end\")\n",
    "\n",
    "    # Decide data source:\n",
    "    # 1) If an image slice is requested, we must use img_psth.\n",
    "    # 2) Else, prefer avg_psth if present; fallback to mean over all images from img_psth.\n",
    "    use_img_slice = (img_start is not None) or (img_end is not None)\n",
    "\n",
    "    if use_img_slice:\n",
    "        if \"img_psth\" not in row or row[\"img_psth\"] is None:\n",
    "            raise ValueError(\"img_psth is required when specifying img_start/img_end.\")\n",
    "        A = np.asarray(row[\"img_psth\"])\n",
    "        if A.ndim != 2:\n",
    "            raise ValueError(f\"img_psth must be 2D (time x images); got ndim={A.ndim}.\")\n",
    "        # Slice images; Python slicing gracefully handles None bounds\n",
    "        A = A[:, img_start:img_end]\n",
    "        if A.size == 0 or A.shape[1] == 0:\n",
    "            raise ValueError(\"Image slice produced empty array; check img_start/img_end.\")\n",
    "        avg = A.mean(axis=1)\n",
    "    else:\n",
    "        if \"avg_psth\" in row and row[\"avg_psth\"] is not None:\n",
    "            avg = np.asarray(row[\"avg_psth\"])\n",
    "            if avg.ndim != 1:\n",
    "                raise ValueError(f\"avg_psth must be 1D; got shape {avg.shape}.\")\n",
    "        else:\n",
    "            if \"img_psth\" not in row or row[\"img_psth\"] is None:\n",
    "                raise ValueError(\"Need either avg_psth (1D) or img_psth (2D).\")\n",
    "            A = np.asarray(row[\"img_psth\"])\n",
    "            if A.ndim != 2:\n",
    "                raise ValueError(f\"img_psth must be 2D (time x images); got ndim={A.ndim}.\")\n",
    "            avg = A.mean(axis=1)\n",
    "\n",
    "    avg = np.asarray(avg).squeeze()\n",
    "    if avg.ndim != 1:\n",
    "        raise ValueError(f\"Resulting average must be 1D; got shape {avg.shape}.\")\n",
    "\n",
    "    # Time slicing\n",
    "    if (t_start is None) and (t_end is None):\n",
    "        out = avg  # full length\n",
    "    else:\n",
    "        # Use Python slice semantics\n",
    "        out = avg[slice(t_start, t_end)]\n",
    "        # If both bounds provided, enforce expected length to catch OOB silently clipped by slicing\n",
    "        if (t_start is not None) and (t_end is not None):\n",
    "            expected = t_end - t_start\n",
    "            if expected <= 0:\n",
    "                raise ValueError(f\"t_end must be > t_start (got {t_start}, {t_end}).\")\n",
    "            if out.shape[0] != expected:\n",
    "                raise ValueError(\n",
    "                    f\"Requested time slice [{t_start}:{t_end}] length {expected}, \"\n",
    "                    f\"but available produced length {out.shape[0]}. \"\n",
    "                    \"Indices may be out of range.\"\n",
    "                )\n",
    "\n",
    "    return out.astype(float, copy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end times calculated from CONFIG \n",
    "start = int(ONSET_MS / BIN_MS) \n",
    "end = start + int(WINDOW_MS / BIN_MS)\n",
    "\n",
    "img_start = None# 1000\n",
    "img_end = None # 1025\n",
    "\n",
    "# If you haven't computed X yet (or want to recompute using a subset of images), do it here:\n",
    "timecourses = []\n",
    "valid_idx = []  # track which unit rows succeed\n",
    "for i, row in dat.iterrows():\n",
    "    try:\n",
    "        # If your helper supports an image subset, use the kwarg (rename as needed):\n",
    "        # tc = get_unit_timecourse(row, start, end, images=IMAGE_SUBSET)\n",
    "        tc = tc = get_unit_timecourse(row, img_start=img_start, img_end=img_end, t_start=start, t_end=end)\n",
    "        timecourses.append(tc)\n",
    "        valid_idx.append(i)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping unit {i}: {e}\")\n",
    "\n",
    "X = np.vstack(timecourses)  # shape: (n_units_valid, T)\n",
    "\n",
    "# Z-score each unit across time; sanitize inf/NaN\n",
    "X = zscore(X, axis=1)\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Helper: metadata\n",
    "# -----------------\n",
    "dat_valid = dat.loc[valid_idx].reset_index(drop=True)  # align 1:1 with rows of X\n",
    "dat_valid[\"roi_clean\"] = dat_valid[\"roi\"].fillna(\"Unknown\")\n",
    "\n",
    "def _sel_from_roi(s):\n",
    "    # last letter (B/O/F) if present; else \"Unknown\"\n",
    "    if isinstance(s, str) and len(s) > 0:\n",
    "        tail = s[-1]\n",
    "        if tail in {\"B\", \"O\", \"F\"}:\n",
    "            return tail\n",
    "    return \"Unknown\"\n",
    "\n",
    "dat_valid[\"sel\"] = dat_valid[\"roi_clean\"].apply(_sel_from_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# KMeans per group (ROI or sel)\n",
    "# ------------------------------\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def kmeans_by_group(\n",
    "    X, meta, group_col, K_RANGE,\n",
    "    random_state=RANDOM_STATE, n_init=N_INIT,\n",
    "    min_units_for_clustering=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs KMeans separately for each group in `group_col`.\n",
    "    Chooses the best K by silhouette (when possible) within each group.\n",
    "    Returns a dict with per-group results and a long df for plotting.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    all_plot_frames = []\n",
    "\n",
    "    T = X.shape[1]\n",
    "    # More correct time axis in ms (bins -> ms). Your original used indices as ms.\n",
    "    # If BIN_MS == 1, this is identical to np.arange(start, end).\n",
    "    time_ms = np.arange(start, end) * BIN_MS\n",
    "\n",
    "    for group_name, idx_series in meta.groupby(group_col).groups.items():\n",
    "        idx = np.array(sorted(list(idx_series)))\n",
    "        Xg = X[idx]\n",
    "        n = Xg.shape[0]\n",
    "\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        # valid Ks for this group: cannot exceed n-1 for silhouette to make sense\n",
    "        valid_ks = [k for k in K_RANGE if 1 < k < n+1]\n",
    "        sil_scores = []\n",
    "        labels_by_k = {}\n",
    "        inertias = {}\n",
    "\n",
    "        if n >= min_units_for_clustering and len(valid_ks) > 0:\n",
    "            for k in valid_ks:\n",
    "                km = KMeans(n_clusters=k, random_state=random_state, n_init=n_init)\n",
    "                labels_k = km.fit_predict(Xg)       # one fit\n",
    "                inertias[k] = km.inertia_\n",
    "                # Only compute silhouette when we have >1 cluster actually formed\n",
    "                if len(np.unique(labels_k)) > 1:\n",
    "                    sil = silhouette_score(Xg, labels_k)\n",
    "                else:\n",
    "                    sil = np.nan\n",
    "                sil_scores.append((k, sil))\n",
    "                labels_by_k[k] = labels_k\n",
    "\n",
    "            # choose K with max silhouette (ignore NaNs); fallback to smallest valid K\n",
    "            if np.all(np.isnan([s for _, s in sil_scores])):\n",
    "                best_k = valid_ks[0]\n",
    "            else:\n",
    "                best_k = max((kv for kv in sil_scores if not np.isnan(kv[1])),\n",
    "                             key=lambda kv: kv[1])[0]\n",
    "            final_labels = labels_by_k[best_k]\n",
    "        else:\n",
    "            # Too few units to cluster: assign a single cluster 0\n",
    "            best_k = 1\n",
    "            inertias[1] = 0.0\n",
    "            final_labels = np.zeros(n, dtype=int)\n",
    "            sil_scores = [(1, np.nan)]\n",
    "\n",
    "        # stash results\n",
    "        results[group_name] = {\n",
    "            \"indices\": idx,                 # which rows of X belong to this group\n",
    "            \"best_k\": best_k,\n",
    "            \"labels\": final_labels,         # cluster labels for rows in this group\n",
    "            \"sil_scores\": sil_scores,\n",
    "            \"inertias\": inertias,\n",
    "        }\n",
    "\n",
    "        # Build long frame for plotting cluster means with CI\n",
    "        df_plot_g = pd.DataFrame({\n",
    "            \"time\": np.tile(time_ms, n),\n",
    "            \"response\": Xg.flatten(),\n",
    "            \"cluster\": np.repeat(final_labels, T),\n",
    "            group_col: group_name\n",
    "        })\n",
    "        all_plot_frames.append(df_plot_g)\n",
    "\n",
    "    df_plot_all = pd.concat(all_plot_frames, ignore_index=True) if all_plot_frames else pd.DataFrame()\n",
    "    return results, df_plot_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Run per-ROI\n",
    "# ---------------\n",
    "roi_results, df_plot_roi = kmeans_by_group(\n",
    "    X, dat_valid, group_col=\"roi_clean\", K_RANGE=K_RANGE,\n",
    "    random_state=RANDOM_STATE, n_init=N_INIT\n",
    ")\n",
    "\n",
    "# ---------------\n",
    "# Run per-selectivity (last letter)\n",
    "# ---------------\n",
    "sel_results, df_plot_sel = kmeans_by_group(\n",
    "    X, dat_valid, group_col=\"sel\", K_RANGE=K_RANGE,\n",
    "    random_state=RANDOM_STATE, n_init=N_INIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Faceted plots (ROI)\n",
    "# --------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def facet_cluster_timecourses(df_long, group_col, title, onset_ms=ONSET_MS, col_wrap=4, height=3.2):\n",
    "    if df_long.empty:\n",
    "        print(f\"No data to plot for {group_col}.\")\n",
    "        return\n",
    "    g = sns.FacetGrid(\n",
    "        df_long, col=group_col, col_wrap=col_wrap, sharey=False, height=height, legend_out=True\n",
    "    )\n",
    "    # map_dataframe passes the subset to seaborn\n",
    "    g.map_dataframe(\n",
    "        sns.lineplot,\n",
    "        x=\"time\", y=\"response\", hue=\"cluster\",\n",
    "        errorbar=(\"se\", 10)  # visual CI; adjust as desired\n",
    "    )\n",
    "    # add onset line to each facet\n",
    "    for ax in g.axes.ravel():\n",
    "        ax.axvline(onset_ms, ls=\"--\", lw=1, color=\"k\", alpha=0.5)\n",
    "        ax.set_xlabel(\"Time (ms)\")\n",
    "        ax.set_ylabel(\"Z-scored response\")\n",
    "    g.add_legend(title=\"Cluster\")\n",
    "    g.set_titles(col_template=\"{col_name}\")\n",
    "    plt.suptitle(title, y=1.02, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "facet_cluster_timecourses(\n",
    "    df_plot_roi, group_col=\"roi_clean\",\n",
    "    title=\"Cluster mean (z-scored) time courses by ROI\"\n",
    ")\n",
    "\n",
    "facet_cluster_timecourses(\n",
    "    df_plot_sel, group_col=\"sel\",\n",
    "    title=\"Cluster mean (z-scored) time courses by selectivity (last letter)\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(dynamics)",
   "language": "python",
   "name": "dynamics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
