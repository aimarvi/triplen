{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2097dc5e-a0c5-4488-83f2-cb432c37a9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-40GB MIG 3g.20gb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(f'you are not using a gpu (or your cuda install is messed up). fix it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a4c402-1547-4bb5-a0db-488b6875c17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /n/home12/amarvi/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233M/233M [00:04<00:00, 58.8MB/s] \n"
     ]
    }
   ],
   "source": [
    "# load pretrained alexnet (imagenet)\n",
    "model = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "\n",
    "# standard imagenet preprocessing\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b82fb57-1fc5-4d75-b872-8daf9356ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(paths):\n",
    "    # returns tensor [n_images, 3, 224, 224]\n",
    "    imgs = [transform(Image.open(p).convert('RGB')) for p in paths]\n",
    "    return torch.stack(imgs)\n",
    "\n",
    "def get_activations(model, x):\n",
    "    activations = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    def hook_fn(name):\n",
    "        def hook(module, inp, out):\n",
    "            # detach so autograd doesn't eat your ram\n",
    "            activations[name] = out.detach()\n",
    "        return hook\n",
    "\n",
    "    # register hooks on all submodules\n",
    "    for name, module in model.named_modules():\n",
    "        if name:  # skip the top-level container\n",
    "            hooks.append(module.register_forward_hook(hook_fn(name)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e604d139-d6f7-4cb9-bb6a-410c7b1cdf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images found: 1072\n"
     ]
    }
   ],
   "source": [
    "# load in NSD and localizer images (total: 1072 images)\n",
    "IMG_DIR = '../../datasets/NNN/NSD1000_LOC'\n",
    "IMG_PATHS = [os.path.join(IMG_DIR, img) for img in os.listdir(IMG_DIR) if '.tsv' not in img]\n",
    "print(f'Total number of images found: {len(IMG_PATHS)}')\n",
    "\n",
    "img_tensor = load_images(IMG_PATHS)\n",
    "acts = get_activations(model, img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef6974fe-505f-4368-a5cc-983464011e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0      |          Conv2d\n",
      "features.1      |            ReLU\n",
      "features.2      |       MaxPool2d\n",
      "features.3      |          Conv2d\n",
      "features.4      |            ReLU\n",
      "features.5      |       MaxPool2d\n",
      "features.6      |          Conv2d\n",
      "features.7      |            ReLU\n",
      "features.8      |          Conv2d\n",
      "features.9      |            ReLU\n",
      "features.10     |          Conv2d\n",
      "features.11     |            ReLU\n",
      "features.12     |       MaxPool2d\n",
      "classifier.0    |         Dropout\n",
      "classifier.1    |          Linear\n",
      "classifier.2    |            ReLU\n",
      "classifier.3    |         Dropout\n",
      "classifier.4    |          Linear\n",
      "classifier.5    |            ReLU\n",
      "classifier.6    |          Linear\n",
      "\n",
      "\n",
      " ================================================== \n",
      "\n",
      "\n",
      "features.0      |      torch.Size([1072, 64, 55, 55])\n",
      "features.1      |      torch.Size([1072, 64, 55, 55])\n",
      "features.2      |      torch.Size([1072, 64, 27, 27])\n",
      "features.3      |     torch.Size([1072, 192, 27, 27])\n",
      "features.4      |     torch.Size([1072, 192, 27, 27])\n",
      "features.5      |     torch.Size([1072, 192, 13, 13])\n",
      "features.6      |     torch.Size([1072, 384, 13, 13])\n",
      "features.7      |     torch.Size([1072, 384, 13, 13])\n",
      "features.8      |     torch.Size([1072, 256, 13, 13])\n",
      "features.9      |     torch.Size([1072, 256, 13, 13])\n",
      "features.10     |     torch.Size([1072, 256, 13, 13])\n",
      "features.11     |     torch.Size([1072, 256, 13, 13])\n",
      "features.12     |       torch.Size([1072, 256, 6, 6])\n",
      "features        |       torch.Size([1072, 256, 6, 6])\n",
      "avgpool         |       torch.Size([1072, 256, 6, 6])\n",
      "classifier.0    |            torch.Size([1072, 9216])\n",
      "classifier.1    |            torch.Size([1072, 4096])\n",
      "classifier.2    |            torch.Size([1072, 4096])\n",
      "classifier.3    |            torch.Size([1072, 4096])\n",
      "classifier.4    |            torch.Size([1072, 4096])\n",
      "classifier.5    |            torch.Size([1072, 4096])\n",
      "classifier.6    |            torch.Size([1072, 1000])\n",
      "classifier      |            torch.Size([1072, 1000])\n"
     ]
    }
   ],
   "source": [
    "# FOR REFERENCE, ALEXNET LAYERS + ACTUAL ACTs\n",
    "for name, module in model.named_modules():\n",
    "    if '.' in name:\n",
    "        print(f'{name:<15} | {module.__class__.__name__:>15}')\n",
    "        \n",
    "print('\\n\\n', '='*50, '\\n\\n')\n",
    "        \n",
    "for layer, out in acts.items():\n",
    "    print(f'{layer:<15} | {str(out.shape):>35}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e10385-068c-43a9-9013-9c2a2acf33c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dynamics]",
   "language": "python",
   "name": "conda-env-.conda-dynamics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
